{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Data Exploration of Seoul Sensor Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_raw = '../data/raw/'\n",
    "filename = 'Measurement_summary.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path_raw+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename some columns so that we can spend less time typing\n",
    "\n",
    "df.rename(columns={\"Measurement date\": \"date\",\n",
    "                   \"Station code\": \"station\",\n",
    "                   \"Address\": \"address\",\n",
    "                   \"Latitude\": \"lat\",\n",
    "                   \"Longitude\": \"lon\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's drop duplicate rows before anything else\n",
    "\n",
    "shape_o = df.shape\n",
    "df.drop_duplicates(keep='first', inplace=True)\n",
    "shape_d = df.shape\n",
    "\n",
    "if shape_o == shape_d:\n",
    "    print(\"The original df contains {} rows and {} columns.\".format(shape_o[0], shape_o[1]),\n",
    "          \"\\nNo duplicate rows need to be dropped.\")\n",
    "else:\n",
    "    print(\"The original df contains {} rows and {} columns.\".format(shape_o[0], shape_o[1]),\n",
    "          \"\\nAfter dropping duplicate rows, the df contains {} rows and {} columns.\".format(shape_d[0],\n",
    "                                                                                            shape_d[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure \"date\" column has the proper datetime format and set it to index\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)\n",
    "df.to_pickle('../data/interim/summary.pkl')\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many stations of data do we have?\n",
    "\n",
    "num_stations = len(df.station.unique())\n",
    "print(\"The number of unique stations in this data set is {}.\".format(num_stations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a bounding box to visualize the sensor spread in Seoul\n",
    "\n",
    "BBox = ((df.lon.min(), df.lon.max(),      \n",
    "         df.lat.min(), df.lat.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map sensors on top of Seoul map image from opestreetmap.org\n",
    "\n",
    "seoul_map = plt.imread('images/Seoul_exact.png')\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8,7))\n",
    "ax.scatter(df.lon, df.lat, zorder=1, alpha= 0.2, c='m', s=10)\n",
    "ax.set_title('Plotting Sensor Locations on Seoul Map')\n",
    "ax.set_xlim(BBox[0],BBox[1])\n",
    "ax.set_ylim(BBox[2],BBox[3])\n",
    "ax.imshow(seoul_map, zorder=0, extent = BBox, aspect= 'equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do we have any nulls or nans?\n",
    "\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many hours per station are missing and which hours?\n",
    "\n",
    "date_rng = pd.date_range(start='1/1/2017', end='1/1/2020', freq='H', closed='left')\n",
    "print(\"The count of datetimes should be {}.\".format(len(date_rng)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_hrs = df[['station', 'lat']].groupby('station').count()\n",
    "df_hrs.rename(columns={\"lat\": \"hrs_available\"}, inplace=True)\n",
    "df_hrs['hrs_missing'] = df_hrs.apply(lambda row: row.hrs_available - len(date_rng), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "for i in range(101,126):\n",
    "    df_del = df[df.station == i]\n",
    "    df_del = df_del.reindex(date_rng, fill_value=(-1-(i-100)))\n",
    "    df_del['hrs_missing'] = df_del.station.where(df_del.station == (-1-(i-100)))\n",
    "    plt.plot(df_del.index, df_del.hrs_missing, label=\"Station {} missing {} hrs\".format(i,-df_hrs.hrs_missing[i]))\n",
    "plt.legend(bbox_to_anchor=(1.02, 0, 1.02, 1), loc='lower left', ncol=1, borderaxespad=0.)\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at some pairplots to see distributions and relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, load masks\n",
    "\n",
    "df_instruments_0 = pd.read_pickle('../data/interim/instrument_mask_0.pkl')\n",
    "df_instruments_1 = pd.read_pickle('../data/interim/instrument_mask_1.pkl')\n",
    "df_instruments_2 = pd.read_pickle('../data/interim/instrument_mask_2.pkl')\n",
    "df_instruments_4 = pd.read_pickle('../data/interim/instrument_mask_4.pkl')\n",
    "df_instruments_8 = pd.read_pickle('../data/interim/instrument_mask_8.pkl')\n",
    "df_instruments_9 = pd.read_pickle('../data/interim/instrument_mask_9.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take subset of pollutant columns from summary dataframe to standardize\n",
    "# change the index so it is compatible with the masks\n",
    "\n",
    "df_to_std = df[['SO2', 'NO2', 'O3','CO','PM10','PM2.5']].copy()\n",
    "\n",
    "df_to_std['idx'] = df_instruments_0.index\n",
    "df_to_std.set_index('idx', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask dataframes\n",
    "\n",
    "df_to_std_0 = df_to_std.copy().where(df_instruments_0)\n",
    "df_to_std_1 = df_to_std.copy().where(df_instruments_1)\n",
    "df_to_std_2 = df_to_std.copy().where(df_instruments_2)\n",
    "df_to_std_4 = df_to_std.copy().where(df_instruments_4)\n",
    "df_to_std_8 = df_to_std.copy().where(df_instruments_8)\n",
    "df_to_std_9 = df_to_std.copy().where(df_instruments_9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize dataframes\n",
    "\n",
    "df_std_all = (df_to_std - df_to_std.mean())/df_to_std.std()\n",
    "df_std_0 = (df_to_std_0 - df_to_std_0.mean())/df_to_std_0.std()\n",
    "df_std_1 = (df_to_std_1 - df_to_std_1.mean())/df_to_std_1.std()\n",
    "df_std_2 = (df_to_std_2 - df_to_std_2.mean())/df_to_std_2.std()\n",
    "df_std_4 = (df_to_std_4 - df_to_std_4.mean())/df_to_std_4.std()\n",
    "df_std_8 = (df_to_std_8 - df_to_std_8.mean())/df_to_std_8.std()\n",
    "df_std_9 = (df_to_std_9 - df_to_std_9.mean())/df_to_std_9.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle dataframes\n",
    "\n",
    "df_to_std.to_pickle('../data/interim/summary_all.pkl')\n",
    "df_to_std_0.to_pickle('../data/interim/summary_0.pkl')\n",
    "df_to_std_1.to_pickle('../data/interim/summary_1.pkl')\n",
    "df_to_std_2.to_pickle('../data/interim/summary_2.pkl')\n",
    "df_to_std_4.to_pickle('../data/interim/summary_4.pkl')\n",
    "df_to_std_8.to_pickle('../data/interim/summary_8.pkl')\n",
    "df_to_std_9.to_pickle('../data/interim/summary_9.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle dataframes\n",
    "\n",
    "df_std_all.to_pickle('../data/interim/summary_std_all.pkl')\n",
    "df_std_0.to_pickle('../data/interim/summary_std_0.pkl')\n",
    "df_std_1.to_pickle('../data/interim/summary_std_1.pkl')\n",
    "df_std_2.to_pickle('../data/interim/summary_std_2.pkl')\n",
    "df_std_4.to_pickle('../data/interim/summary_std_4.pkl')\n",
    "df_std_8.to_pickle('../data/interim/summary_std_8.pkl')\n",
    "df_std_9.to_pickle('../data/interim/summary_std_9.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized values with all instrument statuses\n",
    "\n",
    "sns.pairplot(df_std_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized values with instrument statuses of normal\n",
    "\n",
    "sns.pairplot(df_std_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
